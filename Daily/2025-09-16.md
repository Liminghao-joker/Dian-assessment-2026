**任务**
1. 实现一个基础的self_attention层
2. 实现一个基本的Multi-Head Attention层

**学习**
1. 单头注意力机制原理（query，key，value **QKV**）
2. 动手实现*Multi-Head Attention*

**反思**
1. 注意过程中tensor维度的变化
	其中Q，K，V矩阵的维度相同，`(batch_size, seq_len, d_k)`
	注意力矩阵A的维度，`(batch_size, seq_len, seq_len)`
2. 实现多头注意力维度的变化
```python
d_model # 原始嵌入层的维度
d_k = d_model / num_heads # 每个头嵌入的维度
# Q, K, V
(batch_size, seq_len, d_model) -> (batch_size, num_heads, seq_len, d_k)
# Attention
(batch_size, num_heads, seq_len, seq_len)
# concat X
(batch_size, num_heads, seq_len, d_k) -> (batch_size, seq_len, d_model)

```
**问题**
1. 在实现单头注意力机制时的`dot_production_attention`函数能否适配传入参数带num_heads的情况？
>**Solution** 可以，利用**广播机制**。
